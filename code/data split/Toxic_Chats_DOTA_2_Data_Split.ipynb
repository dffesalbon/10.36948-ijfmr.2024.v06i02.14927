{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet datasets\n",
        "!pip install --quiet emoji\n",
        "!pip install --quiet torchinfo\n",
        "!pip install --quiet imbalanced-learn\n",
        "!pip install --quiet gdown\n",
        "!pip install --quiet clean-text\n",
        "!pip install --quiet accelerate -U"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJhYFtKfx4G5",
        "outputId": "3e3aab85-5a65-4d7a-c718-1a80c9c974cd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.3/519.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m358.9/358.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hTraceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3108, in _dep_map\n",
            "    return self.__dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2901, in __getattr__\n",
            "    raise AttributeError(attr)\n",
            "AttributeError: _DistInfoDistribution__dep_map\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 169, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/req_command.py\", line 242, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 441, in run\n",
            "    conflicts = self._determine_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 572, in _determine_conflicts\n",
            "    return check_install_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 101, in check_install_conflicts\n",
            "    package_set, _ = create_package_set_from_installed()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 42, in create_package_set_from_installed\n",
            "    dependencies = list(dist.iter_dependencies())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/metadata/pkg_resources.py\", line 216, in iter_dependencies\n",
            "    return self._dist.requires(extras)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2821, in requires\n",
            "    dm = self._dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3110, in _dep_map\n",
            "    self.__dep_map = self._compute_dependencies()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3120, in _compute_dependencies\n",
            "    reqs.extend(parse_requirements(req))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3173, in __init__\n",
            "    super(Requirement, self).__init__(requirement_string)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/requirements.py\", line 102, in __init__\n",
            "    req = REQUIREMENT.parseString(requirement_string)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 1131, in parse_string\n",
            "    loc, tokens = self._parse(instring, 0)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4114, in parseImpl\n",
            "    return e._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3864, in parseImpl\n",
            "    loc, resultlist = self.exprs[0]._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3864, in parseImpl\n",
            "    loc, resultlist = self.exprs[0]._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 776, in _parseNoCache\n",
            "    def _parseNoCache(\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1732, in isEnabledFor\n",
            "    return self._cache[level]\n",
            "KeyError: 50\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 79, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n",
            "    return run(options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 206, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1523, in critical\n",
            "    if self.isEnabledFor(CRITICAL):\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1732, in isEnabledFor\n",
            "    return self._cache[level]\n",
            "KeyboardInterrupt\n",
            "^C\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.4/175.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.2/251.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import BCEWithLogitsLoss, MSELoss, CrossEntropyLoss\n",
        "from torch.optim import AdamW\n",
        "from torch.utils import data\n",
        "from torch.utils.data import Dataset as ds, DataLoader\n",
        "\n",
        "# dataset\n",
        "import datasets\n",
        "from datasets import Dataset\n",
        "from datasets import Sequence\n",
        "from datasets import Value\n",
        "from datasets import Features\n",
        "from datasets import ClassLabel\n",
        "from datasets import DatasetDict\n",
        "\n",
        "# sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
        "\n",
        "import re, string\n",
        "import emoji\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pickle\n",
        "import joblib\n",
        "import traceback\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "WDUpIIFNxT2U"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "typ4SJRxxNRZ"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "  text = re.sub('#', '', text) # Removing '#' hashtag\n",
        "  text = re.sub('\\w+:\\/\\/\\S+', '', text) # Removing hyperlink\n",
        "  text = re.sub('[^a-zA-Z]', ' ', text) # Remove punctuation\n",
        "  return text.lower()\n",
        "\n",
        "def compute_metrics(pred):\n",
        "  labels = pred.label_ids\n",
        "  preds = pred.predictions.argmax(-1)\n",
        "  f1 = f1_score(labels, preds, average=\"weighted\")\n",
        "  acc = accuracy_score(labels, preds)\n",
        "  return {\"accuracy\": acc, \"f1\": f1}\n",
        "\n",
        "def create_dataset(df, text, label):\n",
        "  class_names = ['non-toxic','mild', 'severe']\n",
        "  data_dict = {'text':df[text], 'labels':df[label]}\n",
        "  tags = ClassLabel(num_classes=3 , names=class_names)\n",
        "  feature_set = Features({'text':Value(dtype='string'), 'labels':tags})\n",
        "\n",
        "  return Dataset.from_dict(mapping = data_dict, features = feature_set)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataset(random_seed=42):\n",
        "  np.random.seed(random_seed)\n",
        "  torch.manual_seed(random_seed)\n",
        "  data = pd.read_csv('/content/trim.csv')\n",
        "  other_fields = ['date time', 'match', 'region', 'game time', 'slot', 'player slot']\n",
        "  data = data.drop(other_fields, 1)\n",
        "  class_map = {'non-toxic': 0, 'mild': 1, 'severe': 2}\n",
        "  data['target'] = data.apply(lambda x: class_map[x['toxicity']], axis=1)\n",
        "  data = data.drop('toxicity', 1)\n",
        "\n",
        "  #split data, get 25% for testing\n",
        "  train, test = train_test_split(data, test_size = 0.25, random_state = 0)\n",
        "  #clean the data\n",
        "  train.loc[:,'message'] = train['message'].apply(lambda x : clean_text(x))\n",
        "  test.loc[:,'message'] = test['message'].apply(lambda x : clean_text(x))\n",
        "  #split training data, get 10% for validation\n",
        "  train_all_convert = train.copy()\n",
        "  train_all_convert['target'] = train_all_convert['target'].astype(str)\n",
        "  train_all_convert.rename(columns={'message':'prompt', 'target':'completion'}, inplace=True)\n",
        "  train_all_convert.to_json(\"train_all_pandas.jsonl\", orient='records', lines=True)\n",
        "\n",
        "  train, validation = train_test_split(train, test_size=0.1, random_state=random_seed, stratify=train['target'])\n",
        "  train.to_csv('/content/train.csv', index=False)\n",
        "  validation.to_csv('/content/validation.csv', index=False)\n",
        "  test.to_csv('/content/test.csv', index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "eOV84gZjxhxT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_dataset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXg4K1jHydSc",
        "outputId": "070dab9a-7244-4ccb-af4b-2bec8662d43d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-c5ecc2a10ff9>:6: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
            "  data = data.drop(other_fields, 1)\n",
            "<ipython-input-7-c5ecc2a10ff9>:9: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
            "  data = data.drop('toxicity', 1)\n"
          ]
        }
      ]
    }
  ]
}